{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec997813",
   "metadata": {},
   "source": [
    "### 1. MIT Í∏∞Î≥∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93884140",
   "metadata": {},
   "source": [
    "- Ï£ºÎ¨∏Î≥Ñ Ï¥ù Í≤ΩÎ°ú ÏµúÏÜåÌôî"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecf8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from pulp import LpProblem, LpMinimize, LpVariable, lpSum, LpBinary, PULP_CBC_CMD\n",
    "\n",
    "# # Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "# orders = pd.read_csv(\"./data/Sample_InputData.csv\")\n",
    "# od_matrix = pd.read_csv(\"./data/Sample_OD_Matrix.csv\", index_col=0)\n",
    "# parameters = pd.read_csv(\"./data/Sample_Parameters.csv\")\n",
    "\n",
    "# # ÌååÎùºÎØ∏ÌÑ∞\n",
    "# rk = int(parameters.loc[parameters[\"PARAMETERS\"] == \"RK\", \"VALUE\"].values[0])  # ÎûôÎãπ ÏàòÏö© Í∞ÄÎä• SKU Ïàò\n",
    "\n",
    "# # SKU Î∞è Îûô Ï†ïÏùò\n",
    "# skus = orders[\"SKU_CD\"].unique().tolist()\n",
    "# racks = [r for r in od_matrix.columns if r.startswith(\"WP_\")]  # ÎûôÎßå Ï∂îÏ∂ú\n",
    "# orders_grouped = orders.groupby(\"ORD_NO\")[\"SKU_CD\"].apply(list).to_dict()\n",
    "\n",
    "# # Í≤∞Ï†ï Î≥ÄÏàò\n",
    "# x = {(sku, rack): LpVariable(f\"x_{sku}_{rack}\", cat=LpBinary) for sku in skus for rack in racks}\n",
    "# z = {}\n",
    "\n",
    "# # Î¨∏Ï†ú Ï†ïÏùò\n",
    "# prob = LpProblem(\"SKU_Rack_Assignment_MIT\", LpMinimize)\n",
    "\n",
    "# # Î™©Ï†Å Ìï®Ïàò\n",
    "# objective_terms = []\n",
    "\n",
    "# for ord_skus in orders_grouped.values():\n",
    "#     # ÏûÖÍ≥†ÏßÄ ‚Üí Îûô\n",
    "#     for sku in ord_skus:\n",
    "#         for rack in racks:\n",
    "#             dist_start = od_matrix.loc[\"oWP_Start\", rack]\n",
    "#             objective_terms.append(dist_start * x[(sku, rack)])\n",
    "\n",
    "#     # Îûô Í∞Ñ Ïù¥Îèô Í±∞Î¶¨\n",
    "#     for i in range(len(ord_skus)):\n",
    "#         for j in range(i + 1, len(ord_skus)):\n",
    "#             sku_i, sku_j = ord_skus[i], ord_skus[j]\n",
    "#             for rack_i in racks:\n",
    "#                 for rack_j in racks:\n",
    "#                   if rack_i == rack_j :\n",
    "#                     continue\n",
    "#                   z_name = f\"z_{sku_i}_{rack_i}_{sku_j}_{rack_j}\"\n",
    "#                   z_var = LpVariable(z_name, cat=LpBinary)\n",
    "#                   z[(sku_i, rack_i, sku_j, rack_j)] = z_var\n",
    "#                   prob += z_var <= x[(sku_i, rack_i)]\n",
    "#                   prob += z_var <= x[(sku_j, rack_j)]\n",
    "#                   prob += z_var >= x[(sku_i, rack_i)] + x[(sku_j, rack_j)] - 1\n",
    "#                   dist = od_matrix.loc[rack_i, rack_j]\n",
    "#                   objective_terms.append(dist * z_var)\n",
    "\n",
    "#     # Îûô ‚Üí Ï∂úÍ≥†ÏßÄ\n",
    "#     for sku in ord_skus:\n",
    "#         for rack in racks:\n",
    "#             dist_end = od_matrix.loc[rack, \"oWP_End\"]\n",
    "#             objective_terms.append(dist_end * x[(sku, rack)])\n",
    "\n",
    "# # Î™©Ï†Å Ìï®Ïàò ÏÑ§Ï†ï\n",
    "# prob += lpSum(objective_terms)\n",
    "\n",
    "# # Ï†úÏïΩÏ°∞Í±¥ 1: SKUÎäî Ìïú ÎûôÏóêÎßå Î∞∞Ïπò\n",
    "# for sku in skus:\n",
    "#     prob += lpSum(x[(sku, rack)] for rack in racks) == 1\n",
    "\n",
    "# # Ï†úÏïΩÏ°∞Í±¥ 2: ÎûôÎãπ SKU Ïàò Ï†úÌïú\n",
    "# for rack in racks:\n",
    "#     prob += lpSum(x[(sku, rack)] for sku in skus) <= rk\n",
    "\n",
    "# # ÏµúÏ†ÅÌôî ÏàòÌñâ\n",
    "# solver = PULP_CBC_CMD(msg=True, timeLimit=100)\n",
    "# prob.solve(solver)\n",
    "\n",
    "# # Í≤∞Í≥º Ï∂úÎ†•\n",
    "# for (sku, rack), var in x.items():\n",
    "#     if var.value() == 1:\n",
    "#         print(f\"{sku} ‚Üí {rack}\")\n",
    "\n",
    "# # Í≤∞Í≥º Ï†ÄÏû•\n",
    "# sku_to_location = {}\n",
    "\n",
    "# for (sku, rack), var in x.items() :\n",
    "#   if var.value() == 1 :\n",
    "#     sku_to_location[sku] = rack\n",
    "\n",
    "# orders['LOC'] = orders['SKU_CD'].map(sku_to_location)\n",
    "\n",
    "# orders.to_csv('./MIT_basic_output.csv', index= False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e04178",
   "metadata": {},
   "source": [
    "### 2. MIT_ÎπàÎèÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b02126",
   "metadata": {},
   "source": [
    "- 80%ÍπåÏßÄ ÎπàÎèÑ + Ïó∞Í¥ÄÎèÑ Í∏∞Î∞ò + 20% MITÎ°ú Ìï¥ÎèÑ ÏãúÍ∞ÑÏù¥ ÎÑàÎ¨¥ Ïò§Îûò Í±∏Î¶º\n",
    "- 90%ÍπåÏßÄ ÎπàÎèÑ + Ïó∞Í¥ÄÎèÑÍ∏∞Î∞ò + 10% MIT - 30Ï¥à"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6a6e5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully:\n",
      "- Orders: (1426, 6)\n",
      "- Parameters: (5, 3)\n",
      "- OD Matrix: (170, 170)\n",
      "Ï¥ù ÏÉùÏÑ±Îêú ÌÅ¥Îü¨Ïä§ÌÑ∞ Ïàò :  330\n",
      "\n",
      " ‚úÖ Ï¥ù ÌîºÌÇπ ÏãúÍ∞Ñ: 29943.69Ï¥à (ÌîºÌÇπ 1426Ìöå, Ïù¥Îèô Í±∞Î¶¨ 25665.69)\n",
      "\n",
      "Optimization completed. Results preview:\n",
      "      ORD_NO    SKU_CD  NUM_PCS      LOC  CART_NO  SEQ\n",
      "7   ORD_0003  SKU_0057        1  WP_0004        1    1\n",
      "5   ORD_0002  SKU_0005        1  WP_0022        1    2\n",
      "12  ORD_0004  SKU_0037        1  WP_0034        1    3\n",
      "10  ORD_0004  SKU_0097        1  WP_0039        1    4\n",
      "11  ORD_0004  SKU_0180        1  WP_0041        1    5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from pulp import LpProblem, LpMinimize, LpVariable, lpSum, LpBinary, PULP_CBC_CMD\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Set\n",
    "\n",
    "@dataclass\n",
    "class WarehouseParameters:\n",
    "    picking_time: float\n",
    "    walking_time: float\n",
    "    cart_capacity: int\n",
    "    rack_capacity: int\n",
    "    number_pickers: int\n",
    "\n",
    "class WarehouseSolver:\n",
    "    def __init__(self, orders: pd.DataFrame, parameters: pd.DataFrame, od_matrix: pd.DataFrame):\n",
    "        self.orders = orders.copy()\n",
    "        self.params = self._load_parameters(parameters)\n",
    "        self.od_matrix = od_matrix\n",
    "        self.start_location = od_matrix.index[0]\n",
    "        self.end_location = od_matrix.index[1]\n",
    "\n",
    "        self._initialize_orders()\n",
    "        self._validate_input()\n",
    "\n",
    "    def _load_parameters(self, parameters: pd.DataFrame) -> WarehouseParameters:\n",
    "        get_param = lambda x: parameters.loc[parameters['PARAMETERS'] == x, 'VALUE'].iloc[0]\n",
    "        return WarehouseParameters(\n",
    "            picking_time=float(get_param('PT')),\n",
    "            walking_time=float(get_param('WT')),\n",
    "            cart_capacity=int(get_param('CAPA')),\n",
    "            rack_capacity=int(get_param('RK')),\n",
    "            number_pickers=int(get_param('PK'))\n",
    "        )\n",
    "\n",
    "    def _initialize_orders(self) -> None:\n",
    "        self.orders['LOC'] = pd.NA\n",
    "        self.orders['LOC'] = self.orders['LOC'].astype(str)\n",
    "        self.orders['CART_NO'] = pd.NA\n",
    "        self.orders['SEQ'] = pd.NA\n",
    "\n",
    "    def _validate_input(self) -> None:\n",
    "        if self.orders.empty or self.od_matrix.empty:\n",
    "            raise ValueError(\"Input data or OD matrix is empty\")\n",
    "        required_columns = {'ORD_NO', 'SKU_CD'}\n",
    "        if not required_columns.issubset(self.orders.columns):\n",
    "            raise ValueError(f\"Missing required columns: {required_columns - set(self.orders.columns)}\")\n",
    "\n",
    "    def solve_storage_location(self) -> None:\n",
    "        \"\"\"\n",
    "        Solve Storage Location Assignment Problem (SLAP) using MIP\n",
    "        1. ÎπàÎèÑÏàò+ÌÅ¥Îü¨Ïä§ÌÑ∞ Í∏∞Î∞ò 90% Î∞∞Ïπò(ÏûÖÏ∂úÍ≥† ÏßÄÏ†ê Closeness)\n",
    "        2. Ï£ºÎ¨∏Î≥Ñ Ï¥ù Í±∞Î¶¨ ÏµúÏÜåÌôî 10%Î∞∞Ïπò\n",
    "        \"\"\"\n",
    "        rk = self.params.rack_capacity\n",
    "        start_node = self.od_matrix.index[0]\n",
    "        rack_columns = [col for col in self.od_matrix.columns if col.startswith('WP_')]\n",
    "        \n",
    "        # 1. SKU Í∞Ñ ÎèôÏãú Ï∂úÌòÑ ÌñâÎ†¨\n",
    "        orders_by_ordno = self.orders.groupby('ORD_NO')['SKU_CD'].apply(list)\n",
    "        sku_list = sorted(self.orders['SKU_CD'].unique())\n",
    "        sku_index = {sku : idx for idx, sku in enumerate(sku_list)}\n",
    "        co_occurence = np.zeros((len(sku_list), len(sku_list)), dtype= int)\n",
    "        \n",
    "        for sku_group in orders_by_ordno :\n",
    "            for sku1, sku2 in combinations(sku_group, 2) :\n",
    "                idx1, idx2= sku_index[sku1], sku_index[sku2]\n",
    "                co_occurence[idx1][idx2] += 1 # Ìï¥Îãπ skuÏùò Îã§Î•∏ skuÏôÄÏùò Îì±Ïû• ÎπàÎèÑ\n",
    "                co_occurence[idx2][idx1] += 1\n",
    "            \n",
    "            for sku in sku_group :\n",
    "                idx = sku_index[sku]\n",
    "                co_occurence[idx][idx] += 1 # Ìï¥Îãπ skuÏùò Îì±Ïû• ÎπàÎèÑ\n",
    "\n",
    "        co_matrix= pd.DataFrame(co_occurence, index= sku_list, columns= sku_list)\n",
    "        \n",
    "        # 2. ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅ\n",
    "        cosine_dist = cosine_distances(co_matrix) # ÏûêÍ∏∞ ÏûêÏã†Í≥ºÏùò Ïú†ÏÇ¨ÎèÑ 1, Îã§Î•∏ skuÏôÄÏùò Ïú†ÏÇ¨ÎèÑ 0~1\n",
    "        clustering = AgglomerativeClustering(n_clusters= None, metric= 'precomputed', linkage= 'average', distance_threshold= 0.3)\n",
    "        cluster_labels = clustering.fit_predict(cosine_dist)\n",
    "        sku_cluster_map = pd.DataFrame({'SKU' : co_matrix.index, 'Cluster' : cluster_labels})\n",
    "        \n",
    "        print('Ï¥ù ÏÉùÏÑ±Îêú ÌÅ¥Îü¨Ïä§ÌÑ∞ Ïàò : ', sku_cluster_map['Cluster'].nunique())\n",
    "        \n",
    "        # 3. ÌÅ¥Îü¨Ïä§ÌÑ∞ Ï§ë ÎπàÎèÑ ÎåÄÌëú\n",
    "        sku_freq = self.orders['SKU_CD'].value_counts().to_dict()\n",
    "        cluster_freq = sku_cluster_map.copy()\n",
    "        cluster_freq['Frequence'] = cluster_freq['SKU'].map(sku_freq)\n",
    "        cluster_max_freq = cluster_freq.groupby('Cluster')['Frequence'].max().sort_values(ascending= False)\n",
    "        \n",
    "        # 4. Î∞∞ÏπòÌï† ÌÅ¥Îü¨Ïä§ÌÑ∞ ÎπÑÏú® ÏÑ†ÌÉù - 90%\n",
    "        top_n = int(len(cluster_max_freq) * 0.9) or 1\n",
    "        top_clusters = cluster_max_freq.head(top_n).index.tolist()\n",
    "        \n",
    "        priority_skus = sku_cluster_map[sku_cluster_map['Cluster'].isin(top_clusters)]['SKU'].tolist()\n",
    "        n_full_racks = len(priority_skus) // rk\n",
    "        \n",
    "        priority_skus_to_assign = priority_skus[:n_full_racks * rk] # ÏôÑÏ†ÑÌûà Ï±ÑÏö∏ Ïàò ÏûàÎäî skuÎßå\n",
    "        remaining_skus = priority_skus[n_full_racks * rk :] # ÎÇòÎ®∏ÏßÄÎäî MIP\n",
    "        \n",
    "        # 5. ÌÅ¥Îü¨Ïä§ÌÑ∞ ÏûÖÏ∂úÍ≥† ÏßÄÏ†ê Ïù∏Ï†ë ÎûôÏóê Ïö∞ÏÑ† Î∞∞Ïπò\n",
    "        rack_distances = self.od_matrix.loc[start_node, rack_columns].sort_values()\n",
    "        rack_iter = iter(rack_distances.index)\n",
    "        rack_sku_alloctation = defaultdict(list)\n",
    "        current_rack = next(rack_iter)\n",
    "        \n",
    "        for sku in priority_skus_to_assign :\n",
    "            while len(rack_sku_alloctation[current_rack]) >= rk : \n",
    "                current_rack = next(rack_iter)\n",
    "            rack_sku_alloctation[current_rack].append(sku)\n",
    "        \n",
    "        # 6. Í≤∞Í≥º\n",
    "        priority_alloc_df = pd.DataFrame(\n",
    "            [(sku, rack) for rack, skus in rack_sku_alloctation.items() for sku in skus],\n",
    "            columns= ['SKU', 'Assigned_Rack']\n",
    "        )\n",
    "        \n",
    "        # 7. ÎÇ®ÏùÄ SKU / ÎÇ®ÏùÄ ÎûôÏóê MIPÎ°ú Î∞∞Ïπò\n",
    "        orders_by_ordno_dict = self.orders.groupby('ORD_NO')['SKU_CD'].apply(list).to_dict()\n",
    "        \n",
    "        used_skus = set(priority_alloc_df['SKU'])\n",
    "        remaining_skus = sorted(set(sku_list) - used_skus)\n",
    "        used_racks = set(priority_alloc_df['Assigned_Rack'])\n",
    "        remaining_racks = sorted(set(rack_columns) - used_racks)\n",
    "        \n",
    "        x = {(sku, rack): LpVariable(f\"x_{sku}_{rack}\", cat=LpBinary) for sku in remaining_skus for rack in remaining_racks}\n",
    "        z = {}\n",
    "        \n",
    "        prob = LpProblem(\"MIT_Optimization\", LpMinimize)\n",
    "        \n",
    "        objective_terms = []\n",
    "        \n",
    "        for ord_skus in orders_by_ordno_dict.values():\n",
    "            for sku in ord_skus:\n",
    "                if sku in remaining_skus:\n",
    "                    for rack in remaining_racks:\n",
    "                        dist_start = self.od_matrix.loc[\"oWP_Start\", rack]\n",
    "                        objective_terms.append(dist_start * x[(sku, rack)])\n",
    "                elif sku in used_skus:\n",
    "                    rack = priority_alloc_df[priority_alloc_df[\"SKU\"] == sku][\"Assigned_Rack\"].values[0]\n",
    "                    dist_start = self.od_matrix.loc[\"oWP_Start\", rack]\n",
    "                    objective_terms.append(dist_start)\n",
    "\n",
    "            for i in range(len(ord_skus)):\n",
    "                for j in range(i + 1, len(ord_skus)):\n",
    "                    sku_i, sku_j = ord_skus[i], ord_skus[j]\n",
    "                    for rack_i in remaining_racks:\n",
    "                        for rack_j in remaining_racks:\n",
    "                            if rack_i == rack_j:\n",
    "                                continue\n",
    "                            dist = self.od_matrix.loc[rack_i, rack_j]\n",
    "                            if sku_i in remaining_skus and sku_j in remaining_skus:\n",
    "                                z_var = LpVariable(f\"z_{sku_i}_{rack_i}_{sku_j}_{rack_j}\", cat=LpBinary)\n",
    "                                z[(sku_i, rack_i, sku_j, rack_j)] = z_var\n",
    "                                prob += z_var <= x[(sku_i, rack_i)]\n",
    "                                prob += z_var <= x[(sku_j, rack_j)]\n",
    "                                prob += z_var >= x[(sku_i, rack_i)] + x[(sku_j, rack_j)] - 1\n",
    "                                objective_terms.append(dist * z_var)\n",
    "                            elif sku_i in remaining_skus and sku_j in used_skus:\n",
    "                                rack_j_fixed = priority_alloc_df[priority_alloc_df[\"SKU\"] == sku_j][\"Assigned_Rack\"].values[0]\n",
    "                                dist = self.od_matrix.loc[rack_i, rack_j_fixed]\n",
    "                                objective_terms.append(dist * x[(sku_i, rack_i)])\n",
    "                            elif sku_j in remaining_skus and sku_i in used_skus:\n",
    "                                rack_i_fixed = priority_alloc_df[priority_alloc_df[\"SKU\"] == sku_i][\"Assigned_Rack\"].values[0]\n",
    "                                dist = self.od_matrix.loc[rack_i_fixed, rack_j]\n",
    "                                objective_terms.append(dist * x[(sku_j, rack_j)])\n",
    "\n",
    "            for sku in ord_skus:\n",
    "                if sku in remaining_skus:\n",
    "                    for rack in remaining_racks:\n",
    "                        dist_end = self.od_matrix.loc[rack, \"oWP_End\"]\n",
    "                        objective_terms.append(dist_end * x[(sku, rack)])\n",
    "                elif sku in used_skus:\n",
    "                    rack = priority_alloc_df[priority_alloc_df[\"SKU\"] == sku][\"Assigned_Rack\"].values[0]\n",
    "                    dist_end = self.od_matrix.loc[rack, \"oWP_End\"]\n",
    "                    objective_terms.append(dist_end)\n",
    "\n",
    "        prob += lpSum(objective_terms)\n",
    "\n",
    "        # Ï†úÏïΩ Ï°∞Í±¥\n",
    "        for sku in remaining_skus:\n",
    "            prob += lpSum(x[(sku, rack)] for rack in remaining_racks) == 1  # Ìïú SKUÎäî Ìïú ÎûôÏóêÎßå\n",
    "\n",
    "        for rack in remaining_racks:\n",
    "            prob += lpSum(x[(sku, rack)] for sku in remaining_skus) <= rk  # ÎûôÎãπ ÏàòÏö© ÌïúÎèÑ\n",
    "\n",
    "        solver = PULP_CBC_CMD(timeLimit=100, msg=1)  # 1% GAP ÎèÑÎã¨ Ïãú Ï§ëÎã®\n",
    "        prob.solve(solver)\n",
    "\n",
    "        mit_result = {(sku, rack): var.value() for (sku, rack), var in x.items() if var.value() == 1}\n",
    "        mit_alloc_df = pd.DataFrame([(sku, rack) for (sku, rack), v in mit_result.items()],\n",
    "                                    columns=[\"SKU\", \"Assigned_Rack\"])\n",
    "\n",
    "        final_alloc_df = pd.concat([priority_alloc_df, mit_alloc_df], ignore_index=True)\n",
    "        \n",
    "        sku_to_location = dict(zip(final_alloc_df['SKU'], final_alloc_df['Assigned_Rack']))\n",
    "        self.orders['LOC'] = self.orders['SKU_CD'].map(sku_to_location)\n",
    "        \n",
    "\n",
    "    def solve_order_batching(self) -> None:\n",
    "        \"\"\"Solve Order Batching and Sequencing Problem (OBSP) using FIFO strategy\"\"\"\n",
    "        unique_orders = sorted(self.orders['ORD_NO'].unique())\n",
    "        num_carts = len(unique_orders) // self.params.cart_capacity + 1\n",
    "\n",
    "        order_to_cart = {}\n",
    "        for cart_no in range(1, num_carts + 1):\n",
    "            start_idx = (cart_no - 1) * self.params.cart_capacity\n",
    "            end_idx = start_idx + self.params.cart_capacity\n",
    "            cart_orders = unique_orders[start_idx:end_idx]\n",
    "            for order in cart_orders:\n",
    "                order_to_cart[order] = cart_no\n",
    "\n",
    "        self.orders['CART_NO'] = self.orders['ORD_NO'].map(order_to_cart)\n",
    "\n",
    "    def solve_picker_routing(self) -> None:\n",
    "        \"\"\"Solve Pick Routing Problem (PRP) using simple sequencing\"\"\"\n",
    "        self.orders = self.orders.sort_values(['CART_NO', 'LOC'])\n",
    "        self.orders['SEQ'] = self.orders.groupby('CART_NO').cumcount() + 1\n",
    "\n",
    "    def calculate_total_picking_time(self) -> float:\n",
    "        total_walking_distance = 0.0\n",
    "        total_picking_count = len(self.orders)\n",
    "    \n",
    "        for cart_no, group in self.orders.groupby('CART_NO'):\n",
    "            group_sorted = group.sort_values('SEQ')\n",
    "            locations = [self.start_location] + [self.end_location] + group_sorted['LOC'].tolist() \n",
    "    \n",
    "            for i in range(len(locations) - 1):\n",
    "                from_loc, to_loc = locations[i], locations[i + 1]\n",
    "    \n",
    "                # üîç Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏\n",
    "                if from_loc not in self.od_matrix.index or to_loc not in self.od_matrix.columns:\n",
    "                    print(f\"‚ùå Í≤ΩÎ°ú Ï°∞Ìöå Ïò§Î•ò: from={from_loc}, to={to_loc}\")\n",
    "                    raise ValueError(\"OD MatrixÏóê Ï°¥Ïû¨ÌïòÏßÄ ÏïäÎäî ÏúÑÏπòÏûÖÎãàÎã§.\")\n",
    "    \n",
    "                dist = self.od_matrix.loc[from_loc, to_loc]\n",
    "    \n",
    "                # üîç Í±∞Î¶¨ Í∞íÏù¥ NaNÏù¥Î©¥ Í≤ΩÍ≥†\n",
    "                if pd.isna(dist):\n",
    "                    print(f\"‚ùó NaN Í±∞Î¶¨ Î∞úÏÉù: from={from_loc}, to={to_loc}\")\n",
    "                    raise ValueError(\"OD MatrixÏóêÏÑú NaN Í±∞Î¶¨Í∞í Î∞úÍ≤¨\")\n",
    "    \n",
    "                total_walking_distance += dist\n",
    "    \n",
    "        total_time = (\n",
    "            total_walking_distance * self.params.walking_time +\n",
    "            total_picking_count * self.params.picking_time\n",
    "        )\n",
    "        print(f\"\\n ‚úÖ Ï¥ù ÌîºÌÇπ ÏãúÍ∞Ñ: {total_time:.2f}Ï¥à \"\n",
    "              f\"(ÌîºÌÇπ {total_picking_count}Ìöå, Ïù¥Îèô Í±∞Î¶¨ {total_walking_distance:.2f})\")\n",
    "            \n",
    "    def solve(self) -> pd.DataFrame:\n",
    "        \"\"\"Execute complete warehouse optimization solution\"\"\"\n",
    "        self.solve_storage_location()\n",
    "        self.solve_order_batching()\n",
    "        self.solve_picker_routing()\n",
    "        self.calculate_total_picking_time()\n",
    "        return self.orders\n",
    "\n",
    "def main(INPUT: pd.DataFrame, PARAMETER: pd.DataFrame, OD_MATRIX: pd.DataFrame) -> pd.DataFrame:\n",
    "    solver = WarehouseSolver(INPUT, PARAMETER, OD_MATRIX)\n",
    "    return solver.solve()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        test_INPUT = pd.read_csv(\"./data/Sample_InputData.csv\")\n",
    "        test_PARAM = pd.read_csv(\"./data/Sample_Parameters.csv\")\n",
    "        test_OD = pd.read_csv(\"./data/Sample_OD_Matrix.csv\", index_col=0, header=0)\n",
    "\n",
    "        print(\"Data loaded successfully:\")\n",
    "        print(f\"- Orders: {test_INPUT.shape}\")\n",
    "        print(f\"- Parameters: {test_PARAM.shape}\")\n",
    "        print(f\"- OD Matrix: {test_OD.shape}\")\n",
    "\n",
    "        result = main(test_INPUT, test_PARAM, test_OD)\n",
    "        result.to_csv(\"Sample_OutputData.csv\", index=False)\n",
    "        print(\"\\nOptimization completed. Results preview:\")\n",
    "        print(result.head())\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Unable to load required files - {str(e)}\")\n",
    "    except (pd.errors.DataError, pd.errors.EmptyDataError) as e:\n",
    "        print(f\"Error: Data validation failed - {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f61de33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480\n",
      "336\n",
      "168\n"
     ]
    }
   ],
   "source": [
    "temp_df = pd.read_csv('./MIT_Hybrid_Output.csv')\n",
    "temp_df.head()\n",
    "\n",
    "print(temp_df['ORD_NO'].nunique())  #480\n",
    "print(temp_df['SKU_CD'].nunique()) # 336\n",
    "print(temp_df['LOC'].nunique()) # 168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82c955c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pipreqs\n",
      "  Obtaining dependency information for pipreqs from https://files.pythonhosted.org/packages/36/38/cc1343c3a63655e18328e51e00c6e6851be648f1b8babffc5131f1b9f226/pipreqs-0.5.0-py3-none-any.whl.metadata\n",
      "  Downloading pipreqs-0.5.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting docopt==0.6.2 (from pipreqs)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting ipython==8.12.3 (from pipreqs)\n",
      "  Obtaining dependency information for ipython==8.12.3 from https://files.pythonhosted.org/packages/8d/97/8fe103906cd81bc42d3b0175b5534a9f67dccae47d6451131cf8d0d70bb2/ipython-8.12.3-py3-none-any.whl.metadata\n",
      "  Downloading ipython-8.12.3-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting nbconvert<8.0.0,>=7.11.0 (from pipreqs)\n",
      "  Obtaining dependency information for nbconvert<8.0.0,>=7.11.0 from https://files.pythonhosted.org/packages/cc/9a/cd673b2f773a12c992f41309ef81b99da1690426bd2f96957a7ade0d3ed7/nbconvert-7.16.6-py3-none-any.whl.metadata\n",
      "  Downloading nbconvert-7.16.6-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting yarg==0.1.9 (from pipreqs)\n",
      "  Obtaining dependency information for yarg==0.1.9 from https://files.pythonhosted.org/packages/8b/90/89a2ff242ccab6a24fbab18dbbabc67c51a6f0ed01f9a0f41689dc177419/yarg-0.1.9-py2.py3-none-any.whl.metadata\n",
      "  Downloading yarg-0.1.9-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: backcall in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from ipython==8.12.3->pipreqs) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from ipython==8.12.3->pipreqs) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from ipython==8.12.3->pipreqs) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from ipython==8.12.3->pipreqs) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from ipython==8.12.3->pipreqs) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from ipython==8.12.3->pipreqs) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from ipython==8.12.3->pipreqs) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from ipython==8.12.3->pipreqs) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from ipython==8.12.3->pipreqs) (5.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from ipython==8.12.3->pipreqs) (0.4.6)\n",
      "Requirement already satisfied: requests in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from yarg==0.1.9->pipreqs) (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (4.12.2)\n",
      "Requirement already satisfied: bleach[css]!=5.0.0 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (3.1.2)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (5.3.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (0.1.2)\n",
      "Requirement already satisfied: markupsafe>=2.0 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (2.1.1)\n",
      "Collecting mistune<4,>=2.0.3 (from nbconvert<8.0.0,>=7.11.0->pipreqs)\n",
      "  Obtaining dependency information for mistune<4,>=2.0.3 from https://files.pythonhosted.org/packages/01/4d/23c4e4f09da849e127e9f123241946c23c1e30f45a88366879e064211815/mistune-3.1.3-py3-none-any.whl.metadata\n",
      "  Downloading mistune-3.1.3-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (0.5.13)\n",
      "Requirement already satisfied: nbformat>=5.7 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (5.9.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (23.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (1.5.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (0.5.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython==8.12.3->pipreqs) (0.8.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from jupyter-core>=4.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from jupyter-core>=4.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (305.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.5 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (7.4.9)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (1.5.6)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (4.17.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython==8.12.3->pipreqs) (0.2.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from beautifulsoup4->nbconvert<8.0.0,>=7.11.0->pipreqs) (2.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from requests->yarg==0.1.9->pipreqs) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from requests->yarg==0.1.9->pipreqs) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from requests->yarg==0.1.9->pipreqs) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from requests->yarg==0.1.9->pipreqs) (2024.2.2)\n",
      "Requirement already satisfied: executing in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from stack-data->ipython==8.12.3->pipreqs) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from stack-data->ipython==8.12.3->pipreqs) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from stack-data->ipython==8.12.3->pipreqs) (0.2.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (0.18.0)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.5->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.5->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=23.0 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.5->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (23.2.0)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\chjin\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.5->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (6.3.2)\n",
      "Downloading pipreqs-0.5.0-py3-none-any.whl (33 kB)\n",
      "Downloading ipython-8.12.3-py3-none-any.whl (798 kB)\n",
      "   ---------------------------------------- 0.0/798.3 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 399.4/798.3 kB 8.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 737.3/798.3 kB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 798.3/798.3 kB 5.1 MB/s eta 0:00:00\n",
      "Downloading yarg-0.1.9-py2.py3-none-any.whl (19 kB)\n",
      "Downloading nbconvert-7.16.6-py3-none-any.whl (258 kB)\n",
      "   ---------------------------------------- 0.0/258.5 kB ? eta -:--:--\n",
      "   -------------------------------------- - 245.8/258.5 kB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 258.5/258.5 kB 4.0 MB/s eta 0:00:00\n",
      "Downloading mistune-3.1.3-py3-none-any.whl (53 kB)\n",
      "   ---------------------------------------- 0.0/53.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 53.4/53.4 kB 2.7 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13775 sha256=5c933ba226bfafdc5165e37d42b7e76867c6a24f30067d5119145d0a8df5c008\n",
      "  Stored in directory: c:\\users\\chjin\\appdata\\local\\pip\\cache\\wheels\\1a\\b0\\8c\\4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
      "Successfully built docopt\n",
      "Installing collected packages: docopt, mistune, yarg, ipython, nbconvert, pipreqs\n",
      "  Attempting uninstall: mistune\n",
      "    Found existing installation: mistune 0.8.4\n",
      "    Uninstalling mistune-0.8.4:\n",
      "      Successfully uninstalled mistune-0.8.4\n",
      "  Attempting uninstall: ipython\n",
      "    Found existing installation: ipython 8.15.0\n",
      "    Uninstalling ipython-8.15.0:\n",
      "      Successfully uninstalled ipython-8.15.0\n",
      "  Attempting uninstall: nbconvert\n",
      "    Found existing installation: nbconvert 6.5.4\n",
      "    Uninstalling nbconvert-6.5.4:\n",
      "      Successfully uninstalled nbconvert-6.5.4\n",
      "Successfully installed docopt-0.6.2 ipython-8.12.3 mistune-3.1.3 nbconvert-7.16.6 pipreqs-0.5.0 yarg-0.1.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: bleach 4.1.0 does not provide the extra 'css'\n"
     ]
    }
   ],
   "source": [
    "!pip install pipreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7802fda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze > MIT_Frequence_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d11d35d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Not scanning for jupyter notebooks.\n",
      "INFO: Successfully saved requirements file in ./requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!pipreqs ./ --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb8c752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
